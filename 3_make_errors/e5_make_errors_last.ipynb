{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yOqYTZXG5q6"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tjtmddnjswkd/capstone/blob/jc/3_make_errors/e5_make_errors_last.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Poy4ylDO2yJ6"
      },
      "outputs": [],
      "source": [
        "# !pip install kiwipiepy\n",
        "# # !pip install nltk\n",
        "# # !pip install\n",
        "# # !pip install selenium\n",
        "\n",
        "# #코랩에서 구글드라이버 연동\n",
        "# # !apt-get update\n",
        "# # !apt install chromium-chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAOpFDsxG5q_",
        "outputId": "b98a9b7e-65a1-40a3-94a5-3f1ef2337c6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# try:\n",
        "#     from google.colab import drive\n",
        "#     drive.mount('/content/drive')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozBK-UplHXCn"
      },
      "source": [
        "# 0.Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wnd-0dXQG5rB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "\n",
        "if os.path.exists(r\"G:\\내 드라이브\\KB인턴관련\"):\n",
        "    gdrive_path = r\"G:\\내 드라이브\\KB인턴관련\"\n",
        "else:\n",
        "    gdrive_path = \"/content/drive/MyDrive/KB인턴관련\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LQO-CfvlG5rC",
        "outputId": "2db34d01-020a-4731-c918-9360e058e8aa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(gdrive_path, \"preprocessing/fin.csv\"), encoding=\"euc-kr\", index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhPQYnm8LS6o",
        "outputId": "aa562f6f-dc7c-4691-93e3-af7450196929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 280765 entries, 0 to 585940\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count   Dtype \n",
            "---  ------    --------------   ----- \n",
            " 0   category  280765 non-null  object\n",
            " 1   lawName   280765 non-null  object\n",
            " 2   ENG       280765 non-null  object\n",
            " 3   KOR       280765 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 10.7+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa8WyRO-HbDB"
      },
      "source": [
        "# 1.Data Fiiltering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S1Q9BggtIv6n"
      },
      "outputs": [],
      "source": [
        "keyword_idx = df.loc[df.lawName.str.contains('은행|금융|보험|예금|적금|대출|외환|통화|화폐|환율|예대')|df.category.str.contains('경제|금융|상업')].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rhGtK13KJrdk"
      },
      "outputs": [],
      "source": [
        "tgk_idx = df.loc[df.category==\"통화ㆍ국채ㆍ금융\"].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuEzG0xAKzRs",
        "outputId": "c85838a5-be8b-48c5-84f2-5b37bdce1d96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "키워드: 45093\t통화/국채/금융: 16004\n",
            "키워드-통국금: 29089\n",
            "통국금-키워드: 0\n",
            "전체:  45093\n"
          ]
        }
      ],
      "source": [
        "print(f\"키워드: {len(keyword_idx)}\\t통화/국채/금융: {len(tgk_idx)}\")\n",
        "print(f\"키워드-통국금: {len(list(set(keyword_idx)-set(tgk_idx)))}\")\n",
        "print(f\"통국금-키워드: {len(list(set(tgk_idx)-set(keyword_idx)))}\")\n",
        "print(f\"전체:  {len(list(set(keyword_idx).union(set(tgk_idx))))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-eet0uBuLhDt"
      },
      "outputs": [],
      "source": [
        "df_samp = df.loc[list(set(keyword_idx).union(set(tgk_idx)))].drop(index=[403163], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "e7VYU2z46bow",
        "outputId": "ece658e4-9617-4cf9-b3e5-316ff7c60803"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "235673 45092\n"
          ]
        }
      ],
      "source": [
        "nfin_idx = list(set(df.index)-set(df_samp.index))\n",
        "fin_idx = list(df_samp.index)\n",
        "print(len(nfin_idx), len(fin_idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGIRhAJYCEq2"
      },
      "source": [
        "**주요 함수**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "dindex_dict = {\"omission\":[], \"addition\":[], \"mis-translation\":[], \"over-translation\":[], \"under-translation\":[], \"grammer\": []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qtiBdvrdjMGN"
      },
      "outputs": [],
      "source": [
        "## 주요한 함수\n",
        "def remake_df():\n",
        "    result = df.loc[list(set(keyword_idx).union(set(tgk_idx)))]\n",
        "    result[\"curHyp\"] = result.KOR.values\n",
        "    result[\"errors\"] = [[] for _ in range(result.shape[0])]\n",
        "    result[\"errLog\"] = [dict() for _ in range(result.shape[0])]\n",
        "    return result.drop(index=[403163], axis=0)\n",
        "\n",
        "def random_df(dfs, ratio, seed):\n",
        "    np.random.seed(seed)\n",
        "    return dfs.loc[np.random.choice(dfs.index.to_list(), int(dfs.shape[0]*ratio), False), :]\n",
        "\n",
        "def random_index(srs, ratio, seed):\n",
        "    np.random.seed(seed)\n",
        "    return np.random.choice(srs.index, int(srs.shape[0]*ratio), False)\n",
        "\n",
        "def copy_df(data, col=[\"errors\", \"errLog\"]):\n",
        "    result = data.drop(columns=col, axis=1).copy(deep=True)\n",
        "    # print(id(result), id(data))\n",
        "    for c in col:\n",
        "        result[c] = data[c].apply(lambda x: copy.deepcopy(x)).copy(deep=True)\n",
        "    return result\n",
        "\n",
        "def count_tag(tokens, tags=None):\n",
        "    token_dict = dict()\n",
        "    for token in tokens:\n",
        "        if tags!=None and token.tag not in tags: continue\n",
        "        try:\n",
        "            token_dict[token.tag].append(token.form)\n",
        "        except:\n",
        "            token_dict[token.tag] = [token.form]\n",
        "\n",
        "    return token_dict\n",
        "\n",
        "def find_tokens(tokens, tag):\n",
        "    result_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.tag == tag:\n",
        "            result_tokens.append((token.form, token.start, token.end))\n",
        "    return result_tokens\n",
        "\n",
        "def print_df(df, cols, idx=0, end=0):\n",
        "    print(df.shape[0])\n",
        "    if end==0:\n",
        "        values = df.index[idx:]\n",
        "    else:\n",
        "        values = df.index[idx:end]\n",
        "    for i in values:\n",
        "        print(f\"_________________ {i} _________________\")\n",
        "        for col in cols:\n",
        "            print(df.loc[i, col])\n",
        "\n",
        "def input_errors(df, index, severity, start_idx, text, type):\n",
        "    df.loc[index, \"errors\"].append({\"severity\":severity, \"start_idx\":start_idx, \"text\":text, \"type\":type})\n",
        "\n",
        "def convert_df(df, diff_df):\n",
        "    cdf = copy_df(df)\n",
        "    for i in diff_df.index:\n",
        "        cdf.loc[i, : ] = diff_df.loc[i, : ]\n",
        "    return cdf\n",
        "\n",
        "def dup_drop():\n",
        "    for k in dindex_dict.keys():\n",
        "        dindex_dict[k] = list(set(dindex_dict[k]))\n",
        "\n",
        "def check_idx(df):\n",
        "    dup_drop()\n",
        "    for k in dindex_dict.keys():\n",
        "        print(f\"\\t  {k}: {len(dindex_dict[k])}\", end='\\t')\n",
        "        print(df.loc[df.errLog.apply(lambda x: k in x.keys())].shape[0])\n",
        "\n",
        "def check_memorize(data):\n",
        "    df = copy_df(data.loc[:, [\"curHyp\",\"errLog\"]], col=[\"errLog\"])\n",
        "    result_list = []\n",
        "    for i in df.index:\n",
        "        cur_hyp = df.loc[i, \"curHyp\"]\n",
        "        d = df.loc[i, \"errLog\"]\n",
        "        result_dict = dict()\n",
        "        for k in d.keys():\n",
        "            start = d[k][\"start_idx\"]\n",
        "            end = start+len(d[k][\"text\"])\n",
        "            result_dict[k] = cur_hyp[start:end]\n",
        "        result_list.append(result_dict)\n",
        "    df[\"validation\"] = result_list\n",
        "    return df\n",
        "\n",
        "def move_index(df, idx, start, word):\n",
        "    err_log = copy.deepcopy(df.loc[idx, \"errLog\"])\n",
        "    loop_keys = err_log.keys()\n",
        "    for k in loop_keys:\n",
        "        pre_idx = err_log[k][\"start_idx\"]\n",
        "        if pre_idx > start:\n",
        "            df.loc[idx, \"errLog\"][k][\"start_idx\"] = pre_idx + (len(word)+1)\n",
        "\n",
        "def replace_inteli(df, idx, keyword, rword):\n",
        "    errLog = df.loc[idx ,\"errLog\"]\n",
        "    cur_hyp = df.loc[idx, \"curHyp\"]\n",
        "\n",
        "    text_dict=dict()\n",
        "    for k in errLog.keys():\n",
        "        if k==\"omission\": continue\n",
        "        text, sindex = errLog[k][\"text\"], errLog[k][\"start_idx\"]\n",
        "        text_dict[text] = (sindex, sindex+len(text))\n",
        "\n",
        "    reject_range = []\n",
        "    for key in text_dict.keys():\n",
        "        if keyword in key:\n",
        "            reject_range.append(text_dict[key])\n",
        "    try:\n",
        "        for f in re.finditer(keyword, cur_hyp):\n",
        "            start, end = f.start(), f.end()\n",
        "            is_reject = False\n",
        "            for r_s, r_e in reject_range:\n",
        "                if r_s <= start < r_e:\n",
        "                    is_reject=True\n",
        "                elif start <= r_s < r_e <= end:\n",
        "                    is_rejcet=True\n",
        "\n",
        "            if is_reject: continue\n",
        "            while start>=0 and cur_hyp[start]!=\" \":\n",
        "                start-=1\n",
        "            while end<len(cur_hyp) and cur_hyp[end]!=\" \":\n",
        "                end+=1\n",
        "            start += 1\n",
        "\n",
        "            word = cur_hyp[start:end]\n",
        "\n",
        "            if word in text_dict.keys():\n",
        "                start_idx = text_dict[word]\n",
        "                if start==start_idx: continue\n",
        "                break\n",
        "            break\n",
        "    except:\n",
        "        raise print(f\"{keyword} {rword}\\n{cur_hyp}\")\n",
        "\n",
        "    try:\n",
        "        add_word = word.replace(keyword, rword)\n",
        "        conv_sentence = cur_hyp[:start]+add_word+cur_hyp[start+len(word):]\n",
        "        return conv_sentence, start, add_word\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def can_noise(df, i):\n",
        "    cur_hyp = df.loc[i, \"curHyp\"]\n",
        "    errLog = df.loc[i, \"errLog\"]\n",
        "    for k in errLog.keys():\n",
        "        if k==\"omission\": continue\n",
        "        start, text = errLog[k][\"start_idx\"], errLog[k][\"text\"]\n",
        "        end = start+len(text)\n",
        "        cur_hyp = cur_hyp[:start]+(\"ㅱ\"*len(text))+cur_hyp[end:]\n",
        "    return cur_hyp\n",
        "\n",
        "def errors_report(df, rate=False):\n",
        "    sevs = {\"critical\":0, \"major\":1, \"minor\":2, \"neutral\":3}\n",
        "    err_types = {\n",
        "        'omission':[0, 0, 0, 0],\n",
        "        'addition':[0, 0, 0, 0],\n",
        "        'mis-translation':[0, 0, 0, 0],\n",
        "        'over-translation':[0, 0, 0, 0],\n",
        "        'under-translation':[0, 0, 0, 0],\n",
        "        'grammer':[0, 0, 0, 0]\n",
        "        }\n",
        "    for i in df.index:\n",
        "        errors = df.loc[i, \"errors\"]\n",
        "        for d in errors:\n",
        "            err_types[d[\"type\"]][sevs[d[\"severity\"]]]+=1\n",
        "    report_df = pd.DataFrame(err_types, index=sevs.keys())\n",
        "    report_df[\"sum\"] = report_df.apply(np.sum, axis=1)\n",
        "    report_df.loc[\"sum\"] = report_df.apply(np.sum, axis=0)\n",
        "    if rate:\n",
        "        return_df = pd.DataFrame()\n",
        "        for col in report_df.columns:\n",
        "            return_df[col] = report_df[col].apply(lambda x: round((100*x)/report_df.loc[\"sum\", \"sum\"], 2))\n",
        "        return return_df\n",
        "    else:\n",
        "        return report_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_nfin = random_df(df.loc[nfin_idx], 0.3, 111)\n",
        "df_fin = random_df(df.loc[fin_idx], 0.8, 123)\n",
        "df_samp = pd.concat([df_nfin, df_fin], axis=0)\n",
        "df_samp[\"curHyp\"] = df_samp.KOR.values\n",
        "df_samp[\"errors\"] = [[] for _ in range(df_samp.shape[0])]\n",
        "df_samp[\"errLog\"] = [dict() for _ in range(df_samp.shape[0])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9CE0grbeF2H"
      },
      "source": [
        "# 2.품사 태깅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpGSNLUANHMp",
        "outputId": "c33a2683-97d4-4131-ed20-4d0092001a78"
      },
      "outputs": [],
      "source": [
        "# from kiwipiepy import Kiwi\n",
        "\n",
        "# kiwi = Kiwi(num_workers=0, model_type='sbg')\n",
        "# kiwi_tokens = df_samp.KOR.apply(lambda x: kiwi.tokenize(x))\n",
        "# count_tokens = kiwi_tokens.apply(count_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1ZWuJumeRq8",
        "outputId": "54bf313b-0721-4e4e-ba30-afc2bc46f087"
      },
      "outputs": [],
      "source": [
        "# nng_dict = dict()\n",
        "# for tokens in count_tokens.apply(lambda x: x[\"NNG\"]):\n",
        "#     for t in tokens:\n",
        "#         try:\n",
        "#             nng_dict[t] += 1\n",
        "#         except:\n",
        "#             nng_dict[t] = 1\n",
        "# tag = \"NNG\"\n",
        "# nouns =list(set(count_tokens.apply(lambda x: x[tag] if tag in x.keys() else []).sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjsme1DLeVOk"
      },
      "source": [
        "# 3.Error 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0UK0OXjXxyuS"
      },
      "outputs": [],
      "source": [
        "dindex_dict = {\"omission\":[], \"addition\":[], \"mis-translation\":[], \"over-translation\":[], \"under-translation\":[], \"grammer\": []}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3-1.Error 발생 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c-JMkl5WCEq3"
      },
      "outputs": [],
      "source": [
        "# -- Omission : Critical\n",
        "def critical_omission(df, ratio, seed):\n",
        "    df_t = copy_df(df)\n",
        "    cr_om_index = random_index(df_t.curHyp, ratio, seed)\n",
        "    dindex_dict[\"omission\"]+=list(cr_om_index)\n",
        "    result = df_t.loc[cr_om_index]\n",
        "    random_ratio = [3/5, 2/3, 5/8, 5/7]\n",
        "    np.random.seed(seed)\n",
        "    split_sentences = result.curHyp.apply(lambda x: x.split(\" \"))\n",
        "    len_element = split_sentences.apply(len)\n",
        "    crop_element = len_element.apply(lambda x: int(x*(np.random.choice(random_ratio, 1))))\n",
        "    start_idx = (len_element-crop_element).apply(lambda x: np.random.choice([i for i in range(x)], 1)[0])\n",
        "    end_idx = start_idx+crop_element\n",
        "\n",
        "    for i in cr_om_index:\n",
        "        reduce_sentence = (\" \".join([split_sentences[i][k] for k in range(start_idx[i], end_idx[i]+1)])).strip()\n",
        "        cur_hyp = result.loc[i, \"curHyp\"]\n",
        "        sidx = cur_hyp.find(reduce_sentence)\n",
        "\n",
        "        conv_hyp = \" \".join([split_sentences[i][j] for j in range(len_element[i]) if (start_idx[i] > j) or (end_idx[i] < j)])\n",
        "        result.loc[i, \"curHyp\"] = conv_hyp\n",
        "        input_errors(result, i, \"critical\", sidx, reduce_sentence, \"omission\")\n",
        "        result.loc[i, \"errLog\"][\"omission\"] = {\"start_idx\": sidx, \"text\": reduce_sentence}\n",
        "\n",
        "    del split_sentences, len_element, crop_element, start_idx, end_idx\n",
        "    return convert_df(df, result)\n",
        "\n",
        "# -- Omission : Major\n",
        "def major_omission(df_local, ratio, seed, tag, slen, kiwi_tokens):\n",
        "    df_t = copy_df(df_local)\n",
        "    np.random.seed(seed)\n",
        "    nnp_info = kiwi_tokens.apply(lambda x: find_tokens(x, tag)).drop(index=dindex_dict[\"omission\"], axis=0)\n",
        "    np.random.seed(seed)\n",
        "    change_val = nnp_info[nnp_info.apply(len) > 0].apply(lambda x: x[np.random.choice(len(x), 1)[0]])\n",
        "    ch6 = change_val[change_val.apply(lambda x: len(x[0])) > slen]\n",
        "    chv_index = random_index(ch6, ratio, seed)\n",
        "    # drop_idx = ch6.loc[ch6.apply(lambda x: len(x[0].split(\"·\")))>1].index.to_list()\n",
        "    ch_srs = ch6.loc[chv_index]\n",
        "    result = df_t.loc[chv_index]\n",
        "\n",
        "    for i in chv_index:\n",
        "        del_word, start, end = ch_srs.loc[i]\n",
        "        cur_hyp = result.loc[i, \"curHyp\"]\n",
        "\n",
        "        forward = cur_hyp[:start]\n",
        "        backward = cur_hyp[start:]\n",
        "        split_back = backward.split(\" \")\n",
        "        del_unit = split_back[0]\n",
        "        back_mod = \" \".join(split_back[1:])\n",
        "\n",
        "        try:\n",
        "            if start==0:\n",
        "                conv_hyp = (forward+back_mod).strip(\" \")\n",
        "            elif cur_hyp[start-1] == '\"' or cur_hyp[start-1]=='「' or cur_hyp[end]==\"」\" or cur_hyp[end]=='\\\"' :\n",
        "                pattern = f'\\\"[^\\\"]*{del_word}[^\\\"]*\\\"|「[^」]*{del_word}[^」]*」'\n",
        "                find_comp = re.compile(pattern)\n",
        "                find_res = find_comp.search(cur_hyp)\n",
        "                start, end = find_res.span()\n",
        "                while cur_hyp[end]!=\" \" and  end<len(cur_hyp):\n",
        "                    end+=1\n",
        "                while cur_hyp[start]!=\" \" and start>=0:\n",
        "                    start-=1\n",
        "                start+=1\n",
        "                del_unit = cur_hyp[start:end]\n",
        "                conv_hyp = (cur_hyp[:start].strip()+\" \"+cur_hyp[end:].strip()).strip()\n",
        "            elif \"(\" in del_unit:\n",
        "                pattern = f'{del_word}[^\\)]+\\)'\n",
        "                find_comp = re.compile(pattern)\n",
        "                find_res = find_comp.search(cur_hyp)\n",
        "                start, end = find_res.span()\n",
        "                while cur_hyp[end]!=\" \" and  end<len(cur_hyp):\n",
        "                    end+=1\n",
        "                while cur_hyp[start]!=\" \" and start>=0:\n",
        "                    start-=1\n",
        "                start+=1\n",
        "                del_unit = cur_hyp[start:end]\n",
        "                conv_hyp = (cur_hyp[:start].strip()+\" \"+cur_hyp[end:].strip()).strip()\n",
        "            else:\n",
        "                conv_hyp = (forward+back_mod).strip(\" \")\n",
        "            dindex_dict[\"omission\"]+=[i]\n",
        "            result.loc[i, \"curHyp\"] = conv_hyp\n",
        "            input_errors(result, i, \"major\", start, del_unit, \"omission\")\n",
        "            result.loc[i, \"errLog\"][\"omission\"] = {\"start_idx\": start, \"text\": del_unit}\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return convert_df(df_local, result)\n",
        "## -- Omission : Neutral\n",
        "def omission_nt(df, ratio, seed):    # 0.01, 1, 11\n",
        "    df_t = copy_df(df)\n",
        "    np.random.seed(seed)\n",
        "    on_index = random_index(df_t.drop(index=dindex_dict[\"omission\"]), ratio, seed)\n",
        "    on_index = df_t.loc[on_index][df_t.loc[on_index].curHyp.str.contains(r\"\\.|\\;|\\:\")].index\n",
        "    result = df_t.loc[on_index]\n",
        "    dindex_dict[\"omission\"]+=list(on_index)\n",
        "\n",
        "    sp_list = [\".\", \";\", \":\"]\n",
        "    for i in on_index:\n",
        "        np.random.shuffle(sp_list)\n",
        "        for s in sp_list:\n",
        "            cur_hyp = result.loc[i, \"curHyp\"]\n",
        "            s_loc = cur_hyp.find(s)\n",
        "            if s_loc==-1:\n",
        "                continue\n",
        "            conv_hyp = cur_hyp[:s_loc]+cur_hyp[s_loc+1:]\n",
        "            result.loc[i, \"curHyp\"] = conv_hyp\n",
        "            input_errors(result, i, \"neutral\", s_loc, s, \"omission\")\n",
        "            result.loc[i, \"errLog\"][\"omission\"] = {\"start_idx\": s_loc, \"text\": s}\n",
        "            break\n",
        "    return convert_df(df, result)\n",
        "\n",
        "## Addition : Major\n",
        "def major_addition(df, ratio, seed, count_tokens, nouns):\n",
        "    np.random.seed(seed)\n",
        "    df_t=copy_df(df)\n",
        "\n",
        "    ma_ad_index = random_index(df_t, ratio, seed)\n",
        "    nouns_lists = count_tokens[ma_ad_index].apply(lambda x: x[\"NNG\"])\n",
        "    nnpg_info = count_tokens[ma_ad_index].apply(lambda x: [val for val in list(set(x[\"NNG\"])) if len(val)>1])\n",
        "    drop_index = nnpg_info[nnpg_info.apply(len) == 0].index.to_list()\n",
        "    nnpg_info.drop(index=drop_index, axis=0, inplace=True)\n",
        "    ma_ad_index = list(set(ma_ad_index)-set(drop_index))\n",
        "    np.random.seed(seed)\n",
        "    add_position = nnpg_info.apply(lambda x: x[np.random.choice(range(0, len(x)))])\n",
        "    np.random.seed(seed)\n",
        "    add_words = nouns_lists.apply(lambda x: np.random.choice(list(set(nouns)-set(x))))\n",
        "    result = df_t.loc[ma_ad_index]\n",
        "\n",
        "\n",
        "    josa_list = [\"의\", \"에 의한\", \"에\", \"만\", \"만의\"]\n",
        "    for i in ma_ad_index:\n",
        "        cur_hyp = df.loc[i, \"curHyp\"]\n",
        "        josa_add = np.random.choice(josa_list)\n",
        "        word = add_position[i]\n",
        "        start = cur_hyp.find(word)\n",
        "        if word not in cur_hyp:\n",
        "            continue\n",
        "        add_word = add_words[i]+josa_add\n",
        "        while start>=0 and cur_hyp[start]!=\" \":\n",
        "            start-=1\n",
        "        if start < 0:\n",
        "            conv_hyp = f\"{add_word}\"+\" \"+cur_hyp[start+1:].strip()\n",
        "        else:\n",
        "            conv_hyp = cur_hyp[:start+1].strip()+\" \"+f\"{add_word}\"+\" \"+cur_hyp[start+1:].strip()\n",
        "        result.loc[i, \"curHyp\"] = conv_hyp\n",
        "        input_errors(result, i, \"major\", start+1, add_word, \"addition\")\n",
        "\n",
        "        move_index(result, i, start+1, add_word)\n",
        "        result.loc[i, \"errLog\"][\"addition\"] = {\"start_idx\": (start+1), \"text\": add_word}\n",
        "        dindex_dict[\"addition\"].append(i)\n",
        "\n",
        "    return convert_df(df, result)\n",
        "\n",
        "## -- Addition : Minor\n",
        "def ma_iter(df, ratio, seed):   #0.2 2, 34\n",
        "    df_t = copy_df(df)\n",
        "    np.random.seed(seed)\n",
        "    am_index = random_index(df_t.drop(index=dindex_dict[\"addition\"]), ratio, seed)\n",
        "    result = df_t.loc[am_index]\n",
        "    dindex_dict[\"addition\"]+=list(am_index)\n",
        "\n",
        "    for i in am_index:\n",
        "        cur_hyp = result.loc[i, \"curHyp\"]\n",
        "        sch = cur_hyp.split(\" \")\n",
        "        rand_idx = np.random.choice([i for i in range(len(sch))])\n",
        "        random_dup = sch[rand_idx]\n",
        "        conv_hyp=\" \".join(sch[:rand_idx] + [random_dup] +sch[rand_idx:])\n",
        "        if rand_idx==0:\n",
        "            start = 0\n",
        "        else:\n",
        "            start = len(\" \".join(sch[:rand_idx]))+1\n",
        "\n",
        "        result.loc[i, \"curHyp\"] = conv_hyp\n",
        "        input_errors(result, i, \"minor\", start, random_dup, \"addition\")\n",
        "\n",
        "        move_index(result, i, start, random_dup)\n",
        "        result.loc[i, \"errLog\"][\"addition\"] = {\"start_idx\": start, \"text\": random_dup}\n",
        "\n",
        "    return convert_df(df, result)\n",
        "\n",
        "## Mis-Translation : Minor\n",
        "def hv_mistrans(df, ratio, seed):   # 0.3 7, 89\n",
        "    df_t = copy_df(df)\n",
        "    te_drop = df_t.drop(index=dindex_dict[\"mis-translation\"])\n",
        "    transHyp = pd.Series(te_drop.index.map(lambda x: can_noise(te_drop, x)), index=te_drop.index)\n",
        "    temp = te_drop.loc[transHyp.str.contains(r'하여야 한다')]\n",
        "    can_index = random_index(temp, ratio, seed)\n",
        "    result = te_drop.loc[can_index]\n",
        "    ch_words = ['해야만 한다', '해야 할 것이다']\n",
        "    dindex_dict[\"mis-translation\"] += list(can_index)\n",
        "\n",
        "    for i in can_index:\n",
        "        np.random.shuffle(ch_words)\n",
        "        ch_word = ch_words[0]\n",
        "        cur_hyp = result.loc[i, \"curHyp\"]\n",
        "        start = cur_hyp.find('하여야 한다')\n",
        "        conv_hyp = cur_hyp[:start] + ch_word + cur_hyp[start+6:]\n",
        "\n",
        "        result.loc[i, 'curHyp'] = conv_hyp\n",
        "        input_errors(result, i, 'minor', start, ch_word, 'mis-translation')\n",
        "\n",
        "        move_index(result, i, start, ch_word)\n",
        "        result.loc[i, \"errLog\"][\"mis-translation\"] = {\"start_idx\": start, \"text\": ch_word}\n",
        "    return convert_df(df, result)\n",
        "\n",
        "def ddo_trans(df, ratio, seed, val):\n",
        "    df_t = copy_df(df)\n",
        "    conv_list = [\"내지\", \"및\", \"또는\"]\n",
        "    conv_list.remove(val)\n",
        "    te_drop = df_t.drop(index=dindex_dict[\"mis-translation\"])\n",
        "    transHyp = pd.Series(te_drop.index.map(lambda x: can_noise(te_drop, x)), index=te_drop.index)\n",
        "    ival = \" \"+val+\" \"\n",
        "    ddo_idx = random_index(te_drop.loc[transHyp.str.contains(ival)], ratio, seed)\n",
        "    result = te_drop.loc[ddo_idx]\n",
        "    dindex_dict[\"mis-translation\"]+=list(ddo_idx)\n",
        "\n",
        "\n",
        "    for i in ddo_idx:\n",
        "        np.random.shuffle(conv_list)\n",
        "        conv_word = conv_list[0]\n",
        "\n",
        "        conv_hyp, start, fconv_word = replace_inteli(result, i, val, conv_word)\n",
        "\n",
        "        result.loc[i, \"curHyp\"] = conv_hyp\n",
        "        input_errors(result, i, 'minor', start, fconv_word, 'mis-translation')\n",
        "\n",
        "        move_index(result, i, start, fconv_word)\n",
        "        result.loc[i, \"errLog\"][\"mis-translation\"] = {\"start_idx\": start, \"text\": fconv_word}\n",
        "    return convert_df(df, result)\n",
        "\n",
        "## -- Mis-Translation : Major\n",
        "def replace_numbers(df, ratio, seed):\n",
        "    df_t = copy_df(df)\n",
        "    patterns = r\"제[\\d]+[\\w]+[\\d]+[\\w]+\\s\"\n",
        "    compiler = re.compile(patterns)\n",
        "    te_drop = df_t.drop(index=dindex_dict[\"mis-translation\"])\n",
        "    transHyp = pd.Series(te_drop.index.map(lambda x: can_noise(te_drop, x)), index=te_drop.index)\n",
        "    std_idx = random_index(te_drop.loc[transHyp.str.contains(patterns)], ratio, seed)\n",
        "    result = te_drop.loc[std_idx]\n",
        "\n",
        "    for i in std_idx:\n",
        "        cur_hyp = result.loc[i, \"curHyp\"]\n",
        "        all_find = list(compiler.finditer(cur_hyp))\n",
        "        np.random.shuffle(all_find)\n",
        "        for f in all_find:\n",
        "            sts = f.group().strip()\n",
        "            sts_nsp = re.sub(r\"[가-힣]+\", \" \", sts)\n",
        "            sts_list = re.split(r\"[^\\d]+\", sts_nsp)[1:-1]\n",
        "            sts_add = re.split(r\"[\\d]+\", sts)\n",
        "            pre_shuffle = copy.deepcopy(sts_list)\n",
        "            if len(list(set(pre_shuffle)))==1: continue\n",
        "            while True:\n",
        "                np.random.shuffle(pre_shuffle)\n",
        "                if sts_list!=pre_shuffle:\n",
        "                    break\n",
        "        chyp = sts_add.pop(0)\n",
        "        for a, b in zip(sts_add, pre_shuffle):\n",
        "            chyp+=(b+a)\n",
        "\n",
        "        conv_hyp, start, conv_word = replace_inteli(result, i, sts, chyp)\n",
        "\n",
        "        result.loc[i, \"curHyp\"] = conv_hyp\n",
        "        input_errors(result, i, 'major', start, conv_word, 'mis-translation')\n",
        "\n",
        "        move_index(result, i, start, conv_word)\n",
        "        result.loc[i, \"errLog\"][\"mis-translation\"] = {\"start_idx\": start, \"text\": conv_word}\n",
        "        dindex_dict[\"mis-translation\"].append(i)\n",
        "    return convert_df(df, result)\n",
        "\n",
        "## -- Mis-Translation : Neutral\n",
        "def change_morphus(df, ratio, seed, rep_dict):\n",
        "    df_t = copy_df(df)\n",
        "    patterns = \"|\".join(rep_dict.keys())\n",
        "\n",
        "    te_drop = df_t.drop(index=dindex_dict[\"mis-translation\"])\n",
        "    transHyp = pd.Series(te_drop.index.map(lambda x: can_noise(te_drop, x)), index=te_drop.index)\n",
        "    std_idx = random_index(te_drop.loc[transHyp.str.contains(patterns, regex=True)], ratio, seed)\n",
        "\n",
        "    result = te_drop.loc[std_idx]\n",
        "\n",
        "    rpd_keys = copy.deepcopy(list(rep_dict.keys()))\n",
        "    for e, i in enumerate(std_idx):\n",
        "        cur_hyp = result.loc[i, \"curHyp\"]\n",
        "        np.random.shuffle(rpd_keys)\n",
        "        for pattern in rpd_keys:\n",
        "            find_list = list(re.finditer(pattern, cur_hyp))\n",
        "            if not find_list: continue\n",
        "            break\n",
        "\n",
        "        np.random.shuffle(find_list)\n",
        "        f = find_list[0]\n",
        "\n",
        "        word, start = f.group(), f.start()\n",
        "        r_word = (rep_dict[pattern]+word[1:-1]+rep_dict[pattern]).strip()\n",
        "        res = replace_inteli(result, i, word, r_word)\n",
        "        if res!=None:\n",
        "            conv_hyp, start, conv_word = res\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        result.loc[i, \"curHyp\"] = conv_hyp\n",
        "        input_errors(result, i, 'neutral', start, conv_word, 'mis-translation')\n",
        "\n",
        "        move_index(result, i, start, conv_word)\n",
        "        result.loc[i, \"errLog\"][\"mis-translation\"] = {\"start_idx\": start, \"text\": conv_word}\n",
        "        dindex_dict[\"mis-translation\"].append(i)\n",
        "    return convert_df(df, result)\n",
        "\n",
        "def change_word(df, ratio, seed, rep_dict, severity, errType):\n",
        "    df_t = copy_df(df)\n",
        "    patterns = \"|\".join(rep_dict.keys())\n",
        "\n",
        "    te_drop = df_t.drop(index=dindex_dict[errType])\n",
        "    transHyp = pd.Series(te_drop.index.map(lambda x: can_noise(te_drop, x)), index=te_drop.index)\n",
        "    std_idx = random_index(te_drop.loc[transHyp.str.contains(patterns, regex=True)], ratio, seed)\n",
        "\n",
        "    result = te_drop.loc[std_idx]\n",
        "\n",
        "    rpd_keys = copy.deepcopy(list(rep_dict.keys()))\n",
        "    for e, i in enumerate(std_idx):\n",
        "        cur_hyp = result.loc[i, \"curHyp\"]\n",
        "        np.random.shuffle(rpd_keys)\n",
        "        is_complete=False\n",
        "        for pattern in rpd_keys:\n",
        "            i_word = pattern\n",
        "            o_word = rep_dict[pattern][np.random.choice(list(range(0, len(rep_dict[pattern]))))]\n",
        "\n",
        "            find_list = list(re.finditer(pattern, cur_hyp))\n",
        "            if len(find_list)==0: continue\n",
        "            break\n",
        "        np.random.shuffle(find_list)\n",
        "        f = find_list[0]\n",
        "        start, init_word = f.start(), f.group()\n",
        "        res = replace_inteli(result, i, i_word, o_word)\n",
        "        if res != None:\n",
        "            conv_hyp, start, conv_word = res\n",
        "        else:\n",
        "            continue\n",
        "        result.loc[i, \"curHyp\"] = conv_hyp\n",
        "        input_errors(result, i, severity, start, conv_word, errType)\n",
        "\n",
        "        move_index(result, i, start, conv_word)\n",
        "        result.loc[i, \"errLog\"][errType] = {\"start_idx\": start, \"text\": conv_word}\n",
        "        dindex_dict[errType].append(i)\n",
        "    return convert_df(df, result)\n",
        "\n",
        "## -- Grammer\n",
        "def josa_grammer(df, ratio, seed, kiwi):\n",
        "    df_t = copy_df(df)\n",
        "    js_mstr_index = random_index(df_t.drop(index=dindex_dict[\"grammer\"]), ratio, seed)\n",
        "    js_tokens = df_t.loc[js_mstr_index].curHyp.apply(lambda x: kiwi.tokenize(x))\n",
        "\n",
        "    jsd = {\n",
        "        \"가\":\"이\", \"을\":\"를\", \"로써\":\"으로써\", \"로서\":\"으로서\",\n",
        "        \"로\":\"으로\", \"로부터\":\"으로부터\", \"란\":\"이란\", \"나\":\"이나\",\n",
        "        \"와\":\"과\", \"랑\":\"이랑\", \"이랑\":\"랑\", \"며\":\"이며\", \"이며\":\"며\"\n",
        "        }\n",
        "    keys = list(jsd.keys())\n",
        "    for key in keys:\n",
        "        jsd[jsd[key]] = key\n",
        "\n",
        "    tags=[\"JKS\", \"JKC\", \"JKO\", \"JKB\", \"JX\", \"JC\"]\n",
        "    need_josa = None\n",
        "    for i, tag in enumerate(tags):\n",
        "        if i==0:\n",
        "            need_josa = js_tokens.apply(lambda x: find_tokens(x, tag))\n",
        "        else:\n",
        "            need_josa += js_tokens.apply(lambda x: find_tokens(x, tag))\n",
        "\n",
        "    drop_index = need_josa[need_josa.apply(len)==0].index\n",
        "    js_mstr_index = list(set(js_mstr_index)-set(drop_index))\n",
        "    need_josa.drop(index=drop_index, axis=0, inplace=True)\n",
        "    josa_elem = need_josa.apply(lambda x: list(set([val[0] for val in x])))\n",
        "    result = df_t.loc[js_mstr_index]\n",
        "\n",
        "    for i in js_mstr_index:\n",
        "        cur_hyp = result.loc[i, \"curHyp\"]\n",
        "        target = josa_elem[i]\n",
        "        np.random.shuffle(target)\n",
        "        for word in target:\n",
        "            try:\n",
        "                conv_hyp, start, conv_word = replace_inteli(result, i, word, jsd[word])\n",
        "                result.loc[i, \"curHyp\"] = conv_hyp\n",
        "                input_errors(result, i, 'minor', start, conv_word, 'grammer')\n",
        "\n",
        "                move_index(result, i, start, conv_word)\n",
        "                result.loc[i, \"errLog\"][\"grammer\"] = {\"start_idx\": start, \"text\": conv_word}\n",
        "                dindex_dict[\"grammer\"].append(i)\n",
        "                break\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    return convert_df(df, result)\n",
        "\n",
        "def hada_gram(df, ratio, seed):\n",
        "    df_t = copy_df(df)\n",
        "    te_drop = df_t.drop(index=dindex_dict[\"grammer\"])\n",
        "    transHyp = pd.Series(te_drop.index.map(lambda x: can_noise(te_drop, x)), index=te_drop.index)\n",
        "    temp = te_drop.loc[transHyp.apply(lambda x: \"한다\" in x)]\n",
        "    handa_index = random_index(temp, 0.4, 141)\n",
        "    result = df_t.loc[handa_index]\n",
        "    replace_list = [\"하다\", \"함\"]\n",
        "\n",
        "    for i in handa_index:\n",
        "        dindex_dict[\"grammer\"].append(i)\n",
        "        cur_hyp = result.loc[i, \"curHyp\"]\n",
        "        r_word = np.random.choice(replace_list)\n",
        "        conv_hyp, start, conv_word = replace_inteli(result, i, \"한다\", r_word)\n",
        "\n",
        "        result.loc[i, \"curHyp\"] = conv_hyp\n",
        "        input_errors(result, i, 'minor', start, conv_word, 'grammer')\n",
        "\n",
        "        move_index(result, i, start, conv_word)\n",
        "        result.loc[i, \"errLog\"][\"grammer\"] = {\"start_idx\": start, \"text\": conv_word}\n",
        "\n",
        "    return convert_df(df, result)\n",
        "\n",
        "## -- 후처리\n",
        "def convert_index(df_new):\n",
        "    df = copy_df(df_new)\n",
        "    new_errs = []\n",
        "    for i in df.index:\n",
        "        errors, errLog = df.loc[i, \"errors\"], df.loc[i, \"errLog\"]\n",
        "\n",
        "        for i, err in enumerate(df.loc[i, \"errors\"]):\n",
        "            errors[i]['start_idx'] = errLog[err[\"type\"]][\"start_idx\"]\n",
        "\n",
        "        new_errs.append(errors)\n",
        "    df[\"errors\"] = new_errs\n",
        "    return df\n",
        "\n",
        "def make_dict(df):\n",
        "    output_json = dict()\n",
        "    output_json[\"data\"] = []\n",
        "    for i in df.index:\n",
        "        base_form = {\"category\": None, \"en\": None, \"errs\": None, \"hyp\": None, \"ko\": None, \"law_name\": None}\n",
        "        if len(df.loc[i, \"errors\"])==0:\n",
        "            continue\n",
        "        base_form[\"category\"] = df.loc[i, \"category\"]\n",
        "        base_form[\"en\"] = df.loc[i, \"ENG\"]\n",
        "        base_form[\"errs\"] = df.loc[i, \"errors\"]\n",
        "        base_form[\"hyp\"] = df.loc[i, \"curHyp\"]\n",
        "        base_form[\"ko\"] = df.loc[i, \"KOR\"]\n",
        "        base_form[\"law_name\"] = df.loc[i, \"lawName\"]\n",
        "        output_json[\"data\"].append(base_form)\n",
        "    return output_json\n",
        "\n",
        "def export_json(objs, file_name):\n",
        "    with open(os.path.join(gdrive_path, file_name), \"w\") as f:\n",
        "        json.dump(objs, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3-2.사전 정의 필요 변수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(os.path.join(gdrive_path, \"etc_files/under_crawl.pkl\"), \"rb\") as f:\n",
        "    under = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(gdrive_path, \"etc_files/over_crawl.pkl\"), \"rb\") as f:\n",
        "    over = pickle.load(f)\n",
        "    \n",
        "chmorp_dict = {\n",
        "    r'\\\"[^\\\"]+\\\"':\"'\", r\"\\'[^\\']+\\'\":'\"',\n",
        "    r'「[^」]+」':'\"'\n",
        "    }\n",
        "chw_dict={r\"\\s때\":[\" 경우\"], r\"\\s경우\":[\" 때\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([131, 203,  50, 585, 138, 632, 328, 461, 924, 479, 195, 221, 209,\n",
              "       378, 156, 327, 761])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ratios = [\n",
        "    .02, .65, .55, .01, .22,\n",
        "    .2, .3, .2, .25, .3,\n",
        "    .35, .025, .3, .2, .2,\n",
        "    .2 , .4\n",
        "]\n",
        "\n",
        "seed1 = [\n",
        "    10, 20, 36, 111, 55,\n",
        "    234, 789, 555, 444, 333,\n",
        "    987, 777, 908, 808, 707,\n",
        "    1010, 141\n",
        "]\n",
        "\n",
        "np.random.seed(111)\n",
        "seed2 = np.random.choice(1000, 17, replace=False)\n",
        "np.random.seed(123)\n",
        "seed3 = np.random.choice(1000, 17, replace=False)\n",
        "seed3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3-3.한번에 돌리는 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kiwipiepy import Kiwi\n",
        "\n",
        "def lets_make_errors(df, ratios, seeds):\n",
        "    print(\"Tokenizing ...\")\n",
        "    kiwi = Kiwi(num_workers=0, model_type='sbg')\n",
        "    kiwi_tokens = df.KOR.apply(lambda x: kiwi.tokenize(x))\n",
        "    count_tokens = kiwi_tokens.apply(count_tag)\n",
        "\n",
        "    nouns_set = set()\n",
        "    for tokens in count_tokens:\n",
        "        for key in tokens.keys():\n",
        "            if key==\"NNG\":\n",
        "                nouns_set = nouns_set.union(set(tokens[key]))\n",
        "                break\n",
        "    nouns = list(nouns_set)\n",
        "    print(\"Complete Tokenizing !!!\")\n",
        "    \n",
        "    print('\\t1번째 수행 중...', end=\"\")\n",
        "    df_1 = critical_omission(df, ratios[0], seeds[0])\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_1)\n",
        "    print('\\t2번째 수행 중...', end=\"\")\n",
        "    df_2 = major_omission(df_1, ratios[1], seeds[1], \"NNP\", 3, kiwi_tokens)\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_2)\n",
        "    print('\\t3번째 수행 중...', end=\"\")\n",
        "    df_3 = major_omission(df_2, ratios[2], seeds[2], \"NNG\", 2, kiwi_tokens)\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_3)\n",
        "    print('\\t4번째 수행 중...', end=\"\")\n",
        "    df_4 = omission_nt(df_3, ratios[3], seeds[3])\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_4)\n",
        "    print('\\t5번째 수행 중...', end=\"\")\n",
        "    df_5 = major_addition(df_4, ratios[4], seeds[4], count_tokens, nouns)\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_5)\n",
        "    print('\\t6번째 수행 중...', end=\"\")\n",
        "    df_6 = ma_iter(df_5, ratios[5], seeds[5])\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_6)\n",
        "    print('\\t7번째 수행 중...', end=\"\")\n",
        "    df_7 = hv_mistrans(df_6, ratios[6], seeds[6])\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_7)\n",
        "    print('\\t8번째 수행 중...', end=\"\")\n",
        "    df_8 = ddo_trans(df_7, ratios[7], seeds[7], \"또는\")\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_8)\n",
        "    print('\\t9번째 수행 중...', end=\"\")\n",
        "    df_9 = ddo_trans(df_8, ratios[8], seeds[8], \"내지\")\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_9)\n",
        "    print('\\t10번째 수행 중...', end=\"\")\n",
        "    df_10 = ddo_trans(df_9, ratios[9], seeds[9], \"및\")\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_10)\n",
        "    print('\\t11번째 수행 중...', end=\"\")\n",
        "    df_11 = replace_numbers(df_10, ratios[10], seeds[10])\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_11)\n",
        "    print('\\t12번째 수행 중...', end=\"\")\n",
        "    df_12 = change_morphus(df_11, ratios[11], seeds[11], chmorp_dict)\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_12)\n",
        "    print('\\t13번째 수행 중...', end=\"\")\n",
        "    df_13 = change_word(df_12, ratios[12], seeds[12], chw_dict, \"minor\", \"mis-translation\")\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_13)\n",
        "    print('\\t14번째 수행 중...', end=\"\")\n",
        "    df_14 = change_word(df_13, ratios[13], seeds[13], over, \"major\", \"over-translation\")\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_14)\n",
        "    print('\\t15번째 수행 중...', end=\"\")\n",
        "    df_15 = change_word(df_14, ratios[14], seeds[14], under, \"major\", \"under-translation\")\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_15)\n",
        "    print('\\t16번째 수행 중...', end=\"\")\n",
        "    df_16 = josa_grammer(df_15, ratios[15], seeds[15], kiwi)\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_16)\n",
        "    print('\\t17번째 수행 중...', end=\"\")\n",
        "    df_17 = hada_gram(df_16, ratios[16], seeds[16])\n",
        "    print(\"완료 !\")\n",
        "    check_idx(df_17)\n",
        "    df_last = convert_index(df_17.loc[df_17.errors.apply(len)>0])\n",
        "    print(\"Process Done.\")  \n",
        "    \n",
        "    return df_last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_df(df1, df2, s1, s2):\n",
        "    df1 = random_df(df.loc[nfin_idx], 0.3, s1)\n",
        "    df2 = random_df(df.loc[fin_idx], 0.8, s2)\n",
        "    df_samp = pd.concat([df1, df2], axis=0)\n",
        "    df_samp[\"curHyp\"] = df_samp.KOR.values\n",
        "    df_samp[\"errors\"] = [[] for _ in range(df_samp.shape[0])]\n",
        "    df_samp[\"errLog\"] = [dict() for _ in range(df_samp.shape[0])]\n",
        "    return df_samp\n",
        "\n",
        "df_samp1 = make_df(df_nfin, df_fin, 111, 123)\n",
        "df_samp2 = make_df(df_nfin, df_fin, 141, 232)\n",
        "df_samp3 = make_df(df_nfin, df_fin, 2, 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing ...\n",
            "Complete Tokenizing !!!\n",
            "\t1번째 수행 중...완료 !\n",
            "\t  omission: 2135\t2135\n",
            "\t  addition: 0\t0\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t2번째 수행 중...완료 !\n",
            "\t  omission: 8882\t8882\n",
            "\t  addition: 0\t0\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t3번째 수행 중...완료 !\n",
            "\t  omission: 14806\t14806\n",
            "\t  addition: 0\t0\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t4번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 0\t0\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t5번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 23028\t23028\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t6번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 39777\t39777\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t7번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 39777\t39777\n",
            "\t  mis-translation: 6808\t6808\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t8번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 39777\t39777\n",
            "\t  mis-translation: 11764\t11764\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t9번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 39777\t39777\n",
            "\t  mis-translation: 11859\t11859\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t10번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 39777\t39777\n",
            "\t  mis-translation: 18982\t18982\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t11번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 39777\t39777\n",
            "\t  mis-translation: 28815\t28815\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t12번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 39777\t39777\n",
            "\t  mis-translation: 29175\t29175\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t13번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 39777\t39777\n",
            "\t  mis-translation: 38618\t38618\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t14번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 39777\t39777\n",
            "\t  mis-translation: 38618\t38618\n",
            "\t  over-translation: 14998\t14998\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t15번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 39777\t39777\n",
            "\t  mis-translation: 38618\t38618\n",
            "\t  over-translation: 14998\t14998\n",
            "\t  under-translation: 8346\t8346\n",
            "\t  grammer: 0\t0\n",
            "\t16번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 39777\t39777\n",
            "\t  mis-translation: 38618\t38618\n",
            "\t  over-translation: 14998\t14998\n",
            "\t  under-translation: 8346\t8346\n",
            "\t  grammer: 20510\t20510\n",
            "\t17번째 수행 중...완료 !\n",
            "\t  omission: 15611\t15611\n",
            "\t  addition: 39777\t39777\n",
            "\t  mis-translation: 38618\t38618\n",
            "\t  over-translation: 14998\t14998\n",
            "\t  under-translation: 8346\t8346\n",
            "\t  grammer: 40852\t40852\n",
            "Process Done.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>omission</th>\n",
              "      <th>addition</th>\n",
              "      <th>mis-translation</th>\n",
              "      <th>over-translation</th>\n",
              "      <th>under-translation</th>\n",
              "      <th>grammer</th>\n",
              "      <th>sum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>critical</th>\n",
              "      <td>1.35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>major</th>\n",
              "      <td>8.01</td>\n",
              "      <td>14.56</td>\n",
              "      <td>6.22</td>\n",
              "      <td>9.48</td>\n",
              "      <td>5.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>43.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>minor</th>\n",
              "      <td>0.00</td>\n",
              "      <td>10.59</td>\n",
              "      <td>17.97</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>25.82</td>\n",
              "      <td>54.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sum</th>\n",
              "      <td>9.87</td>\n",
              "      <td>25.14</td>\n",
              "      <td>24.41</td>\n",
              "      <td>9.48</td>\n",
              "      <td>5.28</td>\n",
              "      <td>25.82</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          omission  addition  mis-translation  over-translation  \\\n",
              "critical      1.35      0.00             0.00              0.00   \n",
              "major         8.01     14.56             6.22              9.48   \n",
              "minor         0.00     10.59            17.97              0.00   \n",
              "neutral       0.51      0.00             0.23              0.00   \n",
              "sum           9.87     25.14            24.41              9.48   \n",
              "\n",
              "          under-translation  grammer     sum  \n",
              "critical               0.00     0.00    1.35  \n",
              "major                  5.28     0.00   43.54  \n",
              "minor                  0.00    25.82   54.38  \n",
              "neutral                0.00     0.00    0.74  \n",
              "sum                    5.28    25.82  100.00  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dindex_dict = {\"omission\":[], \"addition\":[], \"mis-translation\":[], \"over-translation\":[], \"under-translation\":[], \"grammer\": []}\n",
        "df_res1 = lets_make_errors(df_samp1, ratios, seed1)\n",
        "errors_report(df_res1, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing ...\n",
            "Complete Tokenizing !!!\n",
            "\t1번째 수행 중...완료 !\n",
            "\t  omission: 2135\t2135\n",
            "\t  addition: 0\t0\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t2번째 수행 중...완료 !\n",
            "\t  omission: 8917\t8917\n",
            "\t  addition: 0\t0\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t3번째 수행 중...완료 !\n",
            "\t  omission: 14820\t14820\n",
            "\t  addition: 0\t0\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t4번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 0\t0\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t5번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 23001\t23001\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t6번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 39755\t39755\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t7번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 39755\t39755\n",
            "\t  mis-translation: 6756\t6756\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t8번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 39755\t39755\n",
            "\t  mis-translation: 11762\t11762\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t9번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 39755\t39755\n",
            "\t  mis-translation: 11861\t11861\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t10번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 39755\t39755\n",
            "\t  mis-translation: 18937\t18937\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t11번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 39755\t39755\n",
            "\t  mis-translation: 28794\t28794\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t12번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 39755\t39755\n",
            "\t  mis-translation: 29157\t29157\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t13번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 39755\t39755\n",
            "\t  mis-translation: 38609\t38609\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t14번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 39755\t39755\n",
            "\t  mis-translation: 38609\t38609\n",
            "\t  over-translation: 15020\t15020\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t15번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 39755\t39755\n",
            "\t  mis-translation: 38609\t38609\n",
            "\t  over-translation: 15020\t15020\n",
            "\t  under-translation: 8393\t8393\n",
            "\t  grammer: 0\t0\n",
            "\t16번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 39755\t39755\n",
            "\t  mis-translation: 38609\t38609\n",
            "\t  over-translation: 15020\t15020\n",
            "\t  under-translation: 8393\t8393\n",
            "\t  grammer: 20470\t20470\n",
            "\t17번째 수행 중...완료 !\n",
            "\t  omission: 15641\t15641\n",
            "\t  addition: 39755\t39755\n",
            "\t  mis-translation: 38609\t38609\n",
            "\t  over-translation: 15020\t15020\n",
            "\t  under-translation: 8393\t8393\n",
            "\t  grammer: 40785\t40785\n",
            "Process Done.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>omission</th>\n",
              "      <th>addition</th>\n",
              "      <th>mis-translation</th>\n",
              "      <th>over-translation</th>\n",
              "      <th>under-translation</th>\n",
              "      <th>grammer</th>\n",
              "      <th>sum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>critical</th>\n",
              "      <td>1.35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>major</th>\n",
              "      <td>8.02</td>\n",
              "      <td>14.54</td>\n",
              "      <td>6.23</td>\n",
              "      <td>9.49</td>\n",
              "      <td>5.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>43.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>minor</th>\n",
              "      <td>0.00</td>\n",
              "      <td>10.59</td>\n",
              "      <td>17.94</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>25.78</td>\n",
              "      <td>54.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.52</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sum</th>\n",
              "      <td>9.89</td>\n",
              "      <td>25.13</td>\n",
              "      <td>24.40</td>\n",
              "      <td>9.49</td>\n",
              "      <td>5.31</td>\n",
              "      <td>25.78</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          omission  addition  mis-translation  over-translation  \\\n",
              "critical      1.35      0.00             0.00              0.00   \n",
              "major         8.02     14.54             6.23              9.49   \n",
              "minor         0.00     10.59            17.94              0.00   \n",
              "neutral       0.52      0.00             0.23              0.00   \n",
              "sum           9.89     25.13            24.40              9.49   \n",
              "\n",
              "          under-translation  grammer     sum  \n",
              "critical               0.00     0.00    1.35  \n",
              "major                  5.31     0.00   43.59  \n",
              "minor                  0.00    25.78   54.32  \n",
              "neutral                0.00     0.00    0.75  \n",
              "sum                    5.31    25.78  100.00  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dindex_dict = {\"omission\":[], \"addition\":[], \"mis-translation\":[], \"over-translation\":[], \"under-translation\":[], \"grammer\": []}\n",
        "df_res2 = lets_make_errors(df_samp2, ratios, seed2)\n",
        "errors_report(df_res2, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing ...\n",
            "Complete Tokenizing !!!\n",
            "\t1번째 수행 중...완료 !\n",
            "\t  omission: 2135\t2135\n",
            "\t  addition: 0\t0\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t2번째 수행 중...완료 !\n",
            "\t  omission: 8901\t8901\n",
            "\t  addition: 0\t0\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t3번째 수행 중...완료 !\n",
            "\t  omission: 14767\t14767\n",
            "\t  addition: 0\t0\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t4번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 0\t0\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t5번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 23016\t23016\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t6번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 39767\t39767\n",
            "\t  mis-translation: 0\t0\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t7번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 39767\t39767\n",
            "\t  mis-translation: 6831\t6831\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t8번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 39767\t39767\n",
            "\t  mis-translation: 11783\t11783\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t9번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 39767\t39767\n",
            "\t  mis-translation: 11887\t11887\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t10번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 39767\t39767\n",
            "\t  mis-translation: 18947\t18947\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t11번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 39767\t39767\n",
            "\t  mis-translation: 28806\t28806\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t12번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 39767\t39767\n",
            "\t  mis-translation: 29172\t29172\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t13번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 39767\t39767\n",
            "\t  mis-translation: 38607\t38607\n",
            "\t  over-translation: 0\t0\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t14번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 39767\t39767\n",
            "\t  mis-translation: 38607\t38607\n",
            "\t  over-translation: 15040\t15040\n",
            "\t  under-translation: 0\t0\n",
            "\t  grammer: 0\t0\n",
            "\t15번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 39767\t39767\n",
            "\t  mis-translation: 38607\t38607\n",
            "\t  over-translation: 15040\t15040\n",
            "\t  under-translation: 8420\t8420\n",
            "\t  grammer: 0\t0\n",
            "\t16번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 39767\t39767\n",
            "\t  mis-translation: 38607\t38607\n",
            "\t  over-translation: 15040\t15040\n",
            "\t  under-translation: 8420\t8420\n",
            "\t  grammer: 20461\t20461\n",
            "\t17번째 수행 중...완료 !\n",
            "\t  omission: 15557\t15557\n",
            "\t  addition: 39767\t39767\n",
            "\t  mis-translation: 38607\t38607\n",
            "\t  over-translation: 15040\t15040\n",
            "\t  under-translation: 8420\t8420\n",
            "\t  grammer: 40941\t40941\n",
            "Process Done.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>omission</th>\n",
              "      <th>addition</th>\n",
              "      <th>mis-translation</th>\n",
              "      <th>over-translation</th>\n",
              "      <th>under-translation</th>\n",
              "      <th>grammer</th>\n",
              "      <th>sum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>critical</th>\n",
              "      <td>1.35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>major</th>\n",
              "      <td>7.98</td>\n",
              "      <td>14.54</td>\n",
              "      <td>6.23</td>\n",
              "      <td>9.5</td>\n",
              "      <td>5.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>43.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>minor</th>\n",
              "      <td>0.00</td>\n",
              "      <td>10.58</td>\n",
              "      <td>17.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>25.86</td>\n",
              "      <td>54.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sum</th>\n",
              "      <td>9.83</td>\n",
              "      <td>25.12</td>\n",
              "      <td>24.38</td>\n",
              "      <td>9.5</td>\n",
              "      <td>5.32</td>\n",
              "      <td>25.86</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          omission  addition  mis-translation  over-translation  \\\n",
              "critical      1.35      0.00             0.00               0.0   \n",
              "major         7.98     14.54             6.23               9.5   \n",
              "minor         0.00     10.58            17.93               0.0   \n",
              "neutral       0.50      0.00             0.23               0.0   \n",
              "sum           9.83     25.12            24.38               9.5   \n",
              "\n",
              "          under-translation  grammer     sum  \n",
              "critical               0.00     0.00    1.35  \n",
              "major                  5.32     0.00   43.56  \n",
              "minor                  0.00    25.86   54.36  \n",
              "neutral                0.00     0.00    0.73  \n",
              "sum                    5.32    25.86  100.00  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dindex_dict = {\"omission\":[], \"addition\":[], \"mis-translation\":[], \"over-translation\":[], \"under-translation\":[], \"grammer\": []}\n",
        "df_res3 = lets_make_errors(df_samp3, ratios, seed3)\n",
        "errors_report(df_res3, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "268569\n"
          ]
        }
      ],
      "source": [
        "df_total = pd.concat([df_res1, df_res2, df_res3], axis=0, ignore_index=True)\n",
        "print(df_total.shape[0])\n",
        "# print(df_total.drop_duplicates(subset=[\"errors\"]).shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "265964"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_nd = df_total.drop_duplicates(subset=[\"curHyp\"])\n",
        "df_nd.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1: 44.02\n",
            "2: 37.91\n",
            "3: 15.01\n",
            "4: 2.83\n",
            "5: 0.22\n",
            "6: 0.0\n"
          ]
        }
      ],
      "source": [
        "for i in range(1,7):\n",
        "    print(f\"{i}: {round(100*(df_nd.errors.apply(len)==i).sum()/df_nd.shape[0], 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n",
            "_________________ 6760 _________________\n",
            "행정관청은 제1항의 인공구조물로서 소하성어류의 통로에 방해가 된다고 인정하면 그 인공구조물의 소유자·점유자 또는 시설자에 대하여 방해를 제거하기 위하여 필요한 공사를 명할 수 있다.\n",
            "제1항의 인공구조물로서 소하성어류의 통로에 방해이 된다고 된다고 인정하면 그 인공구조물의 건물소유자·점유자 및 시설자에 대하여 방해를 제거하기 위하여 필요한 공기업를 명할 수 있다.\n",
            "[{'severity': 'major', 'start_idx': 0, 'text': '행정관청은', 'type': 'omission'}, {'severity': 'minor', 'start_idx': 32, 'text': '된다고', 'type': 'addition'}, {'severity': 'minor', 'start_idx': 72, 'text': '및', 'type': 'mis-translation'}, {'severity': 'major', 'start_idx': 54, 'text': '건물소유자·점유자', 'type': 'over-translation'}, {'severity': 'major', 'start_idx': 92, 'text': '공기업를', 'type': 'under-translation'}, {'severity': 'minor', 'start_idx': 24, 'text': '방해이', 'type': 'grammer'}]\n",
            "{'omission': {'start_idx': 0, 'text': '행정관청은'}, 'addition': {'start_idx': 32, 'text': '된다고'}, 'mis-translation': {'start_idx': 72, 'text': '및'}, 'over-translation': {'start_idx': 54, 'text': '건물소유자·점유자'}, 'under-translation': {'start_idx': 92, 'text': '공기업를'}, 'grammer': {'start_idx': 24, 'text': '방해이'}}\n",
            "_________________ 24150 _________________\n",
            "특허청장은 법 제56조제2항에 따라 다음 각 호의 업무를 한국발명진흥회에 위탁한다.\n",
            "채권국에 의한 특허정부위원은 특별법 제2조제56항에 따라 다음 각 호의 업무을 위탁한다.\n",
            "[{'severity': 'major', 'start_idx': 65, 'text': '한국발명진흥회에', 'type': 'omission'}, {'severity': 'major', 'start_idx': 0, 'text': '채권국에 의한', 'type': 'addition'}, {'severity': 'major', 'start_idx': 28, 'text': '제2조제56항에', 'type': 'mis-translation'}, {'severity': 'major', 'start_idx': 22, 'text': '특별법', 'type': 'over-translation'}, {'severity': 'major', 'start_idx': 8, 'text': '특허정부위원은', 'type': 'under-translation'}, {'severity': 'minor', 'start_idx': 40, 'text': '업무을', 'type': 'grammer'}]\n",
            "{'omission': {'start_idx': 65, 'text': '한국발명진흥회에'}, 'addition': {'start_idx': 0, 'text': '채권국에 의한'}, 'mis-translation': {'start_idx': 28, 'text': '제2조제56항에'}, 'over-translation': {'start_idx': 22, 'text': '특별법'}, 'under-translation': {'start_idx': 8, 'text': '특허정부위원은'}, 'grammer': {'start_idx': 40, 'text': '업무을'}}\n",
            "_________________ 24840 _________________\n",
            "특별자치시장·특별자치도지사·시장·군수·구청장은 게임물 관련사업자가 다음 각 호의 어느 하나에 해당하여 영업정지처분을 하여야 하는 때에는 대통령령이 정하는 바에 따라 그 영업정지처분에 갈음하여 10억원 이하의 과징금을 부과할 수 있다. 다만, 제3호는 제26조제2항에 따라 정보통신망을 통한 게임제공업 등록을 한 자에 한정하여 적용한다.\n",
            "특별자치5일장·게임물 관련사업자이 다음 각 호의 어느 하나에 해당하여 영업정지처분을 하여야 하는 때에는 국가원수령이 정하는 바에 따라 그 영업정지처분에 갈음하여 10억원 이하의 과징금을 부과할 수 있다. 다만, 제3호는 제2조제26항에 따라 정보통신망을 통한 게임제공업 등록을 등록을 한 자에 한정하여 적용한다.\n",
            "[{'severity': 'major', 'start_idx': 26, 'text': '특별자치도지사·시장·군수·구청장은', 'type': 'omission'}, {'severity': 'minor', 'start_idx': 184, 'text': '등록을', 'type': 'addition'}, {'severity': 'major', 'start_idx': 147, 'text': '제2조제26항에', 'type': 'mis-translation'}, {'severity': 'major', 'start_idx': 0, 'text': '특별자치5일장·게임물', 'type': 'over-translation'}, {'severity': 'major', 'start_idx': 65, 'text': '국가원수령이', 'type': 'under-translation'}, {'severity': 'minor', 'start_idx': 12, 'text': '관련사업자이', 'type': 'grammer'}]\n",
            "{'omission': {'start_idx': 26, 'text': '특별자치도지사·시장·군수·구청장은'}, 'addition': {'start_idx': 184, 'text': '등록을'}, 'mis-translation': {'start_idx': 147, 'text': '제2조제26항에'}, 'over-translation': {'start_idx': 0, 'text': '특별자치5일장·게임물'}, 'under-translation': {'start_idx': 65, 'text': '국가원수령이'}, 'grammer': {'start_idx': 12, 'text': '관련사업자이'}}\n",
            "_________________ 37724 _________________\n",
            "제4항제1호의 사항은 회장을 거쳐 특별자치시장·특별자치도지사 또는 시장·군수·구청장의 인가를 받지 아니하면 그 효력을 발생하지 아니한다.\n",
            "제1항제4호의 사항은 회장을 거쳐 거쳐 특별자치5일장·또는 시장·군수·구정부위원의 인가을 받지 아니하면 그 효력을 발생하지 아니한다.\n",
            "[{'severity': 'major', 'start_idx': 65, 'text': '특별자치도지사', 'type': 'omission'}, {'severity': 'minor', 'start_idx': 52, 'text': '거쳐', 'type': 'addition'}, {'severity': 'major', 'start_idx': 0, 'text': '제1항제4호의', 'type': 'mis-translation'}, {'severity': 'major', 'start_idx': 22, 'text': '특별자치5일장·또는', 'type': 'over-translation'}, {'severity': 'major', 'start_idx': 33, 'text': '시장·군수·구정부위원의', 'type': 'under-translation'}, {'severity': 'minor', 'start_idx': 46, 'text': '인가을', 'type': 'grammer'}]\n",
            "{'omission': {'start_idx': 65, 'text': '특별자치도지사'}, 'addition': {'start_idx': 52, 'text': '거쳐'}, 'mis-translation': {'start_idx': 0, 'text': '제1항제4호의'}, 'over-translation': {'start_idx': 22, 'text': '특별자치5일장·또는'}, 'under-translation': {'start_idx': 33, 'text': '시장·군수·구정부위원의'}, 'grammer': {'start_idx': 46, 'text': '인가을'}}\n",
            "_________________ 52462 _________________\n",
            "변호사는 폐업하려면 소속 지방변호사회를 거쳐 대한변호사협회에 등록취소를 신청하여야 한다.\n",
            "위장폐업하려면 소속 지방변호사회를 지방변호사회을 거쳐 대한변호사등록법인에 등록취소를 신청해야만 한다.\n",
            "[{'severity': 'major', 'start_idx': 0, 'text': '변호사는', 'type': 'omission'}, {'severity': 'minor', 'start_idx': 17, 'text': '지방변호사회를', 'type': 'addition'}, {'severity': 'minor', 'start_idx': 72, 'text': '해야만 한다', 'type': 'mis-translation'}, {'severity': 'major', 'start_idx': 0, 'text': '위장폐업하려면', 'type': 'over-translation'}, {'severity': 'major', 'start_idx': 38, 'text': '대한변호사등록법인에', 'type': 'under-translation'}, {'severity': 'minor', 'start_idx': 19, 'text': '지방변호사회을', 'type': 'grammer'}]\n",
            "{'omission': {'start_idx': 0, 'text': '변호사는'}, 'addition': {'start_idx': 17, 'text': '지방변호사회를'}, 'mis-translation': {'start_idx': 72, 'text': '해야만 한다'}, 'over-translation': {'start_idx': 0, 'text': '위장폐업하려면'}, 'under-translation': {'start_idx': 38, 'text': '대한변호사등록법인에'}, 'grammer': {'start_idx': 19, 'text': '지방변호사회을'}}\n",
            "_________________ 119519 _________________\n",
            "법 제58조의2에 따른 한국폐기물협회(이하 \"협회\"라 한다)에 총회, 이사회 및 사무국을 둔다.\n",
            "특별법 제2조의58에 따른 한국폐기물협회(이하 \"협회\"라 한다)에 국제연합, 및 사무국를 사무국을 둔다.\n",
            "[{'severity': 'major', 'start_idx': 62, 'text': '이사회', 'type': 'omission'}, {'severity': 'minor', 'start_idx': 64, 'text': '사무국을', 'type': 'addition'}, {'severity': 'major', 'start_idx': 6, 'text': '제2조의58에', 'type': 'mis-translation'}, {'severity': 'major', 'start_idx': 0, 'text': '특별법', 'type': 'over-translation'}, {'severity': 'major', 'start_idx': 37, 'text': '국제연합,', 'type': 'under-translation'}, {'severity': 'minor', 'start_idx': 45, 'text': '사무국를', 'type': 'grammer'}]\n",
            "{'omission': {'start_idx': 62, 'text': '이사회'}, 'addition': {'start_idx': 64, 'text': '사무국을'}, 'mis-translation': {'start_idx': 6, 'text': '제2조의58에'}, 'over-translation': {'start_idx': 0, 'text': '특별법'}, 'under-translation': {'start_idx': 37, 'text': '국제연합,'}, 'grammer': {'start_idx': 45, 'text': '사무국를'}}\n",
            "_________________ 138002 _________________\n",
            "고용노동부장관은 제14조(제15조의2제4항에 따라 준용되는 경우를 포함한다)에 따라 확정된 시정명령을 이행할 의무가 있는 사용자의 사업 또는 사업장에서 해당 시정명령의 효력이 미치는 근로자 이외의 기간제근로자 또는 단시간근로자에 대하여 차별적 처우가 있는지를 조사하여 차별적 처우가 있는 경우에는 그 시정을 요구할 수 있다.\n",
            "고용노동부장관은 제14조(제15조의2제4항에 따라 준용되는 경우를 포함함)에 따라 확정된 시정성문법을 이행할 의무가 있는 사용자의 가공사업 내지 사업장에서 분쇄만의 해당 시정명령의 효력이 미치는 근로자 이외의 기간제또는 단시간근로자에 대하여 차별적 처우가 있는지를 조사하여 차별적 처우가 있는 경우에는 그 시정을 요구할 수 있다.\n",
            "[{'severity': 'major', 'start_idx': 139, 'text': '근로자', 'type': 'omission'}, {'severity': 'major', 'start_idx': 106, 'text': '분쇄만의', 'type': 'addition'}, {'severity': 'minor', 'start_idx': 94, 'text': '내지', 'type': 'mis-translation'}, {'severity': 'major', 'start_idx': 86, 'text': '가공사업', 'type': 'over-translation'}, {'severity': 'major', 'start_idx': 57, 'text': '시정성문법을', 'type': 'under-translation'}, {'severity': 'minor', 'start_idx': 37, 'text': '포함함)에', 'type': 'grammer'}]\n",
            "{'omission': {'start_idx': 139, 'text': '근로자'}, 'addition': {'start_idx': 106, 'text': '분쇄만의'}, 'mis-translation': {'start_idx': 94, 'text': '내지'}, 'over-translation': {'start_idx': 86, 'text': '가공사업'}, 'under-translation': {'start_idx': 57, 'text': '시정성문법을'}, 'grammer': {'start_idx': 37, 'text': '포함함)에'}}\n",
            "_________________ 158883 _________________\n",
            "이 법에 따른 협회가 아닌 자는 한국화재보험협회 또는 이와 유사한 명칭을 사용하지 못한다.\n",
            "이 특별법에 따른 등록법인가 아닌 자는 및 이와 자조에 유사한 명칭을 사용하지 못함.\n",
            "[{'severity': 'major', 'start_idx': 29, 'text': '한국화재보험협회', 'type': 'omission'}, {'severity': 'major', 'start_idx': 37, 'text': '자조에', 'type': 'addition'}, {'severity': 'minor', 'start_idx': 29, 'text': '및', 'type': 'mis-translation'}, {'severity': 'major', 'start_idx': 2, 'text': '특별법에', 'type': 'over-translation'}, {'severity': 'major', 'start_idx': 10, 'text': '등록법인가', 'type': 'under-translation'}, {'severity': 'minor', 'start_idx': 44, 'text': '못함.', 'type': 'grammer'}]\n",
            "{'omission': {'start_idx': 29, 'text': '한국화재보험협회'}, 'addition': {'start_idx': 37, 'text': '자조에'}, 'mis-translation': {'start_idx': 29, 'text': '및'}, 'over-translation': {'start_idx': 2, 'text': '특별법에'}, 'under-translation': {'start_idx': 10, 'text': '등록법인가'}, 'grammer': {'start_idx': 44, 'text': '못함.'}}\n",
            "_________________ 205363 _________________\n",
            "제1항에 따른 시정명령을 받은 시·도지사 또는 시장·군수·구청장은 특별한 사유가 없으면 이에 따라야 한다. 이 경우 제1항제2호 또는 제3호에 따라 재심의 명령을 받은 경우에는 해당 명령을 받은 날부터 15일 이내에 건축위원회의 심의를 하여야 한다.\n",
            "제1항에 따른 시정대통령령을 받은 시·도지사 또는 시장·군수·구청장은 특별한 사유가 없으면 이에 따라야 하다. 이 경우 제1항제2호 또는 제3호에 따라 재심의 성문법을 받은 경우에는 후방에 의한 해당 명령을 받은 날부터 15일 이내에 건축심의를 해야만 한다.\n",
            "[{'severity': 'major', 'start_idx': 147, 'text': '위원회의', 'type': 'omission'}, {'severity': 'major', 'start_idx': 116, 'text': '후방에 의한', 'type': 'addition'}, {'severity': 'minor', 'start_idx': 151, 'text': '해야만 한다', 'type': 'mis-translation'}, {'severity': 'major', 'start_idx': 8, 'text': '시정대통령령을', 'type': 'over-translation'}, {'severity': 'major', 'start_idx': 93, 'text': '성문법을', 'type': 'under-translation'}, {'severity': 'minor', 'start_idx': 58, 'text': '하다.', 'type': 'grammer'}]\n",
            "{'omission': {'start_idx': 147, 'text': '위원회의'}, 'addition': {'start_idx': 116, 'text': '후방에 의한'}, 'mis-translation': {'start_idx': 151, 'text': '해야만 한다'}, 'over-translation': {'start_idx': 8, 'text': '시정대통령령을'}, 'under-translation': {'start_idx': 93, 'text': '성문법을'}, 'grammer': {'start_idx': 58, 'text': '하다.'}}\n",
            "_________________ 211147 _________________\n",
            "이 법에 따른 보상금을 받을 자가 같은 원인에 대하여 다른 법률에 따라 손해배상을 받은 경우에는 그 범위에서 보상금을 지급하지 아니한다.\n",
            "이 특별법에 따른 보상금를 받을 보험 같은 원인에 대하여 다른 법률에 따라 손해배상을 받은 경우에는 그 그 범위에서 보상금을 지급하지 아니한다\n",
            "[{'severity': 'neutral', 'start_idx': 95, 'text': '.', 'type': 'omission'}, {'severity': 'minor', 'start_idx': 72, 'text': '그', 'type': 'addition'}, {'severity': 'minor', 'start_idx': 62, 'text': '경우에는', 'type': 'mis-translation'}, {'severity': 'major', 'start_idx': 2, 'text': '특별법에', 'type': 'over-translation'}, {'severity': 'major', 'start_idx': 23, 'text': '보험', 'type': 'under-translation'}, {'severity': 'minor', 'start_idx': 10, 'text': '보상금를', 'type': 'grammer'}]\n",
            "{'omission': {'start_idx': 95, 'text': '.'}, 'addition': {'start_idx': 72, 'text': '그'}, 'mis-translation': {'start_idx': 62, 'text': '경우에는'}, 'over-translation': {'start_idx': 2, 'text': '특별법에'}, 'under-translation': {'start_idx': 23, 'text': '보험'}, 'grammer': {'start_idx': 10, 'text': '보상금를'}}\n",
            "_________________ 211995 _________________\n",
            "운전면허시험의 장소는 도로교통공단이 정한다. 다만, 법 제83조제1항 각 호 외의 부분 단서 및 이 영 제43조제1항에 따라 시·도경찰청장이나 도로교통공단이 실시하는 원동기장치자전거 면허시험의 경우에는 시·도경찰청장이나 도로교통공단이 정하여 공고한다.\n",
            "운전면허시험의 서적의 장소는 도로교통공단가 정한다. 다만, 특별법 제83조제1항 각 호 외의 부분 단서 및 이 영 제43조제1항에 따라 시·도경찰정부위원이나 도로교통공단이 실시하는 면허시험의 경우에는 시·도경찰청장이나 도로교통공단이 정하여 공고한다.\n",
            "[{'severity': 'major', 'start_idx': 121, 'text': '원동기장치자전거', 'type': 'omission'}, {'severity': 'major', 'start_idx': 8, 'text': '서적의', 'type': 'addition'}, {'severity': 'minor', 'start_idx': 127, 'text': '경우에는', 'type': 'mis-translation'}, {'severity': 'major', 'start_idx': 41, 'text': '특별법', 'type': 'over-translation'}, {'severity': 'major', 'start_idx': 84, 'text': '시·도경찰정부위원이나', 'type': 'under-translation'}, {'severity': 'minor', 'start_idx': 16, 'text': '도로교통공단가', 'type': 'grammer'}]\n",
            "{'omission': {'start_idx': 121, 'text': '원동기장치자전거'}, 'addition': {'start_idx': 8, 'text': '서적의'}, 'mis-translation': {'start_idx': 127, 'text': '경우에는'}, 'over-translation': {'start_idx': 41, 'text': '특별법'}, 'under-translation': {'start_idx': 84, 'text': '시·도경찰정부위원이나'}, 'grammer': {'start_idx': 16, 'text': '도로교통공단가'}}\n",
            "_________________ 212782 _________________\n",
            "신고대상 가축의 진단이나 검안을 의뢰받은 수의사등은 검사 결과를 지체 없이 당사자에게 통보하여야 하고 검사 결과 가축전염병으로 확인된 경우에는 수의사등과 그 신고대상 가축의 소유자등은 지체 없이 국립가축방역기관장, 신고대상 가축의 소재지를 관할하는 시장·군수·구청장 또는 시·도 가축방역기관장에게 신고하여야 한다.\n",
            "신고대상 가축의 진단이나 검안을 의뢰받은 수의료인등은 인성검사 중장년만 결과를 지체 없이 당사자에게 통보하여야 하고 검사 결과 가축전염병으로 확인된 경우에는 수의사등과 그 신고대상 가축의 소유자등은 지체 없이 국립가축방역기관장, 신고대상 가축의 관할하는 시장·군수·구청장 또는 시·도 가축방역기관장에게 신고해야만 함.\n",
            "[{'severity': 'major', 'start_idx': 146, 'text': '소재지를', 'type': 'omission'}, {'severity': 'major', 'start_idx': 44, 'text': '중장년만', 'type': 'addition'}, {'severity': 'minor', 'start_idx': 183, 'text': '해야만 한다', 'type': 'mis-translation'}, {'severity': 'major', 'start_idx': 36, 'text': '인성검사', 'type': 'over-translation'}, {'severity': 'major', 'start_idx': 23, 'text': '수의료인등은', 'type': 'under-translation'}, {'severity': 'minor', 'start_idx': 175, 'text': '함.', 'type': 'grammer'}]\n",
            "{'omission': {'start_idx': 146, 'text': '소재지를'}, 'addition': {'start_idx': 44, 'text': '중장년만'}, 'mis-translation': {'start_idx': 183, 'text': '해야만 한다'}, 'over-translation': {'start_idx': 36, 'text': '인성검사'}, 'under-translation': {'start_idx': 23, 'text': '수의료인등은'}, 'grammer': {'start_idx': 175, 'text': '함.'}}\n"
          ]
        }
      ],
      "source": [
        "print_df(df_nd[df_nd.errors.apply(len)==6].iloc[:, 3:], [\"KOR\", \"curHyp\", \"errors\", \"errLog\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp = check_memorize(df_nd)\n",
        "\n",
        "result = []\n",
        "for errType in dindex_dict.keys():\n",
        "    ttp = temp.loc[temp.errLog.apply(lambda i: errType in i)]\n",
        "    result.append(ttp.loc[ttp[\"errLog\"].apply(lambda x: x[errType][\"text\"]) != ttp[\"validation\"].apply(lambda y: y[errType])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>curHyp</th>\n",
              "      <th>errLog</th>\n",
              "      <th>validation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>응답에 의한 국토교통부장관(법 제82조에 따라 국토교통부장관의 권한를 위임·위탁받은...</td>\n",
              "      <td>{'addition': {'start_idx': 0, 'text': '응답에 의한'...</td>\n",
              "      <td>{'addition': '응답에 의한', 'mis-translation': ' ',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>관광단지을 개발하려는 공공기관 등 문화체육관광부령으로 정하는 관광단지개발자는 필요하...</td>\n",
              "      <td>{'addition': {'start_idx': 128, 'text': '지속에 의...</td>\n",
              "      <td>{'addition': ')를 국가원', 'mis-translation': '54조...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>유료도로관리권자는 통행료와 부이통행료를 낼 의무가 있는 자가 이를 내지 아니하면 국...</td>\n",
              "      <td>{'addition': {'start_idx': 116, 'text': '또는'},...</td>\n",
              "      <td>{'addition': '장(', 'under-translation': ' 정하는 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>사회복지법인(이하 사회복지법인(이하 이 장에서 \"법인\"이라 한다)를 설립하려는 자는...</td>\n",
              "      <td>{'addition': {'start_idx': 0, 'text': '사회복지법인(...</td>\n",
              "      <td>{'addition': '사회복지법인(이하', 'under-translation':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>심사관은 다음 각 호의 어느 하나에 해당하는 경우에는 출원인에게 미리 거절이유(제5...</td>\n",
              "      <td>{'addition': {'start_idx': 95, 'text': '통지하여야'...</td>\n",
              "      <td>{'addition': '야 통지해', 'mis-translation': ' 것이다...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267919</th>\n",
              "      <td>법 제77조제1항에 따른 합병증 등 재요양 사유가 발생할 우려가 있는 자(이하 '합...</td>\n",
              "      <td>{'mis-translation': {'start_idx': 44, 'text': ...</td>\n",
              "      <td>{'mis-translation': ''합병증등예방관리대상자'라', 'under-t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268059</th>\n",
              "      <td>법 제81조제1항제2호이목 단서에서 \"명령으로 정하는 경우\"란 고위험군에 집합투자기...</td>\n",
              "      <td>{'addition': {'start_idx': 56, 'text': '고위험군에'...</td>\n",
              "      <td>{'addition': '는 해산되', 'under-translation': '란 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268131</th>\n",
              "      <td>제114조제1항 또는 제118조에 따른 보고를 하지 아니하거이나 거짓된 보고를 한 ...</td>\n",
              "      <td>{'under-translation': {'start_idx': 68, 'text'...</td>\n",
              "      <td>{'under-translation': '르지 아', 'grammer': '아니하거...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268156</th>\n",
              "      <td>이 법와 다른 법률에서 우선구매 대상으로 규정한 중소기업제품이나 기업제품이나 수의계...</td>\n",
              "      <td>{'addition': {'start_idx': 30, 'text': '중소기업제품...</td>\n",
              "      <td>{'addition': '업제품이나 기업', 'under-translation': ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268438</th>\n",
              "      <td>해산의 결의·합병와 보험계약의 담합에 이전은 금융위원회의 예방적감독행위를 받아야 한다.</td>\n",
              "      <td>{'addition': {'start_idx': 24, 'text': '담합에'},...</td>\n",
              "      <td>{'addition': ' 금융', 'under-translation': '를 받아...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2245 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   curHyp  \\\n",
              "59      응답에 의한 국토교통부장관(법 제82조에 따라 국토교통부장관의 권한를 위임·위탁받은...   \n",
              "167     관광단지을 개발하려는 공공기관 등 문화체육관광부령으로 정하는 관광단지개발자는 필요하...   \n",
              "173     유료도로관리권자는 통행료와 부이통행료를 낼 의무가 있는 자가 이를 내지 아니하면 국...   \n",
              "234     사회복지법인(이하 사회복지법인(이하 이 장에서 \"법인\"이라 한다)를 설립하려는 자는...   \n",
              "357     심사관은 다음 각 호의 어느 하나에 해당하는 경우에는 출원인에게 미리 거절이유(제5...   \n",
              "...                                                   ...   \n",
              "267919  법 제77조제1항에 따른 합병증 등 재요양 사유가 발생할 우려가 있는 자(이하 '합...   \n",
              "268059  법 제81조제1항제2호이목 단서에서 \"명령으로 정하는 경우\"란 고위험군에 집합투자기...   \n",
              "268131  제114조제1항 또는 제118조에 따른 보고를 하지 아니하거이나 거짓된 보고를 한 ...   \n",
              "268156  이 법와 다른 법률에서 우선구매 대상으로 규정한 중소기업제품이나 기업제품이나 수의계...   \n",
              "268438   해산의 결의·합병와 보험계약의 담합에 이전은 금융위원회의 예방적감독행위를 받아야 한다.   \n",
              "\n",
              "                                                   errLog  \\\n",
              "59      {'addition': {'start_idx': 0, 'text': '응답에 의한'...   \n",
              "167     {'addition': {'start_idx': 128, 'text': '지속에 의...   \n",
              "173     {'addition': {'start_idx': 116, 'text': '또는'},...   \n",
              "234     {'addition': {'start_idx': 0, 'text': '사회복지법인(...   \n",
              "357     {'addition': {'start_idx': 95, 'text': '통지하여야'...   \n",
              "...                                                   ...   \n",
              "267919  {'mis-translation': {'start_idx': 44, 'text': ...   \n",
              "268059  {'addition': {'start_idx': 56, 'text': '고위험군에'...   \n",
              "268131  {'under-translation': {'start_idx': 68, 'text'...   \n",
              "268156  {'addition': {'start_idx': 30, 'text': '중소기업제품...   \n",
              "268438  {'addition': {'start_idx': 24, 'text': '담합에'},...   \n",
              "\n",
              "                                               validation  \n",
              "59      {'addition': '응답에 의한', 'mis-translation': ' ',...  \n",
              "167     {'addition': ')를 국가원', 'mis-translation': '54조...  \n",
              "173     {'addition': '장(', 'under-translation': ' 정하는 ...  \n",
              "234     {'addition': '사회복지법인(이하', 'under-translation':...  \n",
              "357     {'addition': '야 통지해', 'mis-translation': ' 것이다...  \n",
              "...                                                   ...  \n",
              "267919  {'mis-translation': ''합병증등예방관리대상자'라', 'under-t...  \n",
              "268059  {'addition': '는 해산되', 'under-translation': '란 ...  \n",
              "268131  {'under-translation': '르지 아', 'grammer': '아니하거...  \n",
              "268156  {'addition': '업제품이나 기업', 'under-translation': ...  \n",
              "268438  {'addition': ' 금융', 'under-translation': '를 받아...  \n",
              "\n",
              "[2245 rows x 3 columns]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_nd.to_csv(os.path.join(gdrive_path, \"tot_res1.csv\"), encoding=\"euc-kr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_crom = critical_omission(df_samp, 0.02, 10)\n",
        "# df_maom = major_omission(df_crom, 0.65, 20, \"NNP\", 3)\n",
        "# df_mo = major_omission(df_maom, 0.55, 36, \"NNG\", 2)\n",
        "# df_on = omission_nt(df_mo, 0.01, 111)\n",
        "# df_maad = major_addition(df_on, 0.22, 55)\n",
        "# df_miter = ma_iter(df_maad, 0.2, 234)\n",
        "# df_can = hv_mistrans(df_miter, 0.3, 789)\n",
        "# df_ddo = ddo_trans(df_can, 0.2, 555, \"또는\")\n",
        "# df_nae = ddo_trans(df_ddo, 0.25, 444, \"내지\")\n",
        "# df_mit = ddo_trans(df_nae, 0.3, 333, \"및\")\n",
        "# df_rpnm = replace_numbers(df_mit, 0.35, 987)\n",
        "# df_cmp = change_morphus(df_rpnm, 0.025, 777, chmorp_dict)\n",
        "# df_ch1 = change_word(df_cmp, 0.3, 908, chw_dict, \"minor\", \"mis-translation\")\n",
        "# df_over = change_word(df_ch1, 0.2, 808, over, \"major\", \"over-translation\")\n",
        "# df_under = change_word(df_over, 0.2, 707, under, \"major\", \"under-translation\")\n",
        "# df_jmt = josa_grammer(df_under, 0.2, 1010)\n",
        "# df_hada = hada_gram(df_jmt, 0.4, 141)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.내보내기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "json_dict = make_dict(df_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "export_json(json_dict, \"data_0828.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "main",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
